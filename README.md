# 7.Bee_Haven
Case Study â€“ Bee Haven Data Lakehouse (Completed)

Objective: Revived a multi-year bee health monitoring project by integrating legacy hive sensor data with live streams into a unified Azure-based data lakehouse.
Project Goal: Built a scalable, end-to-end pipeline in Azure Data Lake Gen2 + Data Factory + Synapse that merged historical CSV logs, real-time JSON telemetry, and external weather APIsâ€”delivering clean, queryable insights for researchers at the University of Honighausen.
Key Achievements:
Designed a medallion architecture (Bronze â†’ Silver â†’ Gold) to handle raw, cleaned, and analytical layers.
Created dynamic ADF pipelines using Get Metadata + ForEach to auto-ingest hundreds of legacy files regardless of naming or schema drift.
Transformed semi-structured JSON hive events and enriched with OpenWeather API data using Synapse Spark notebooks.
Enforced RBAC/IAM security across storage, factory, and analytics workspacesâ€”zero open buckets, full audit trail.

Focus: Real-world ETL/ELT orchestration in the cloudâ€”because bees donâ€™t wait for bad data engineering.

Steps Completed:

ğŸ— Created Data Lake with Hierarchical Namespaces
ğŸ— Built First Data Factory Pipeline
ğŸ— Deployed Fully Dynamic Ingestion Pipeline
ğŸ— Cleaned and Standardized Hive Sensor Data
ğŸ— Joined Hive + Weather on Timestamp/Location
